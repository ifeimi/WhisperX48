{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisperX48_local: 用于本地运行WhisperX48的Jupyter Notebook脚本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "z0igG7ruI-7q",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载whisper模型 Loading whisper model...\n",
      "识别中 Transcribe in progress...\n",
      "加载调整模型 Load alignment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\transformers\\configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调整识别结果 Align whisper output...\n",
      "识别完毕 Done\n",
      "Time consumpution 26.63913893699646s\n",
      "字幕生成完毕 All done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ffmpeg\n",
    "import subprocess\n",
    "import torch\n",
    "import whisperx\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "model_size = \"small\"  # @param [\"base\",\"small\",\"medium\", \"large\"]\n",
    "language = \"ja\"  # @param {type:\"string\"}\n",
    "#sub_style = \"default\"  # @param [\"default\", \"ikedaCN\", \"kaedeCN\",\"sugawaraCN\"]\n",
    "#compression_ratio_threshold = 2.4 # @param {type:\"number\"}\n",
    "#no_speech_threshold = 0.6 # @param {type:\"number\"}\n",
    "#logprob_threshold = -1.0 # @param {type:\"number\"}\n",
    "#condition_on_previous_text = \"True\" # @param [\"True\", \"False\"]\n",
    "\n",
    "output_dir = \"./files/\"  # 默认的音频文件输入和字幕文件输出路径 Path for input audio file and output subtitle by default\n",
    "file_name = \"sample1.wav\"  # 在这里输入音频文件名 Name of the audio file\n",
    "audio_file = output_dir + file_name\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.cuda.empty_cache()\n",
    "print('加载whisper模型 Loading whisper model...')\n",
    "model = whisperx.load_model(model_size, device)\n",
    "\n",
    "#Original whisper transcribe\n",
    "tic = time.time()\n",
    "print('识别中 Transcribe in progress...')\n",
    "result = model.transcribe(audio_file, language =language)\n",
    "\n",
    "#Load alignment model and metadata\n",
    "print('加载调整模型 Load alignment model...')\n",
    "#model_id = \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\"\n",
    "model_a, metadata = whisperx.load_align_model(language_code=\"ja\", device=device)\n",
    "\n",
    "#Align whisper output\n",
    "print('调整识别结果 Align whisper output...')\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device)\n",
    "\n",
    "toc = time.time()\n",
    "print('识别完毕 Done')\n",
    "print(f'Time consumpution {toc-tic}s')\n",
    "\n",
    "#Write SRT file\n",
    "from whisperx.utils import write_srt\n",
    "with open(Path(output_dir) / (audio_file + \".srt\"), \"w\", encoding=\"utf-8\") as srt:\n",
    "    write_srt(result[\"segments\"], file=srt)\n",
    "print('字幕生成完毕 Subtitle generated!')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Last modified 2023-03-07\n",
    "* Author: ifeimi &#11046 Email me: yfwu0202 AT gmail.com\n",
    "\n",
    "* Acknowledgements and copyright notice: \n",
    "This script relies on [whisperx](https://github.com/m-bain/whisperX), which provides an improvement to [OpenAI's whisper](https://github.com/openai/whisper) with more accurate and especially word-level timestamps. This is achieved by forcing align the inaccurate timestamps generated by whisper with some speech model ([wav2vec2.0](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self) for example). \n",
    "\n",
    "Part of this code was referenced from [N46Whisper](https://github.com/Ayanaminn/N46Whisper) project under [MIT license](https://github.com/ifeimi/WhisperX48/blob/main/LICENSE). Modifications were made to incorporate the usage of [whisperx](https://github.com/m-bain/whisperX). "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
